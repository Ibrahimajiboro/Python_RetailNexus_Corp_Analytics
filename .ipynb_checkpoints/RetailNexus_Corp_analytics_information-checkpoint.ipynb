{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3781af09-09e4-46a7-b85e-9ed603dd47ac",
   "metadata": {},
   "source": [
    "## folder structure"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3dd0037-44cb-4ef7-b848-4191261791cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "project/\n",
    "├── data/\n",
    "│   ├── raw/\n",
    "│   ├── processed/\n",
    "├── notebooks/\n",
    "│   └── eda.ipynb\n",
    "├── scripts/\n",
    "│   ├── data_loader.py\n",
    "│   ├── data_cleaner.py\n",
    "│   ├── transformer.py\n",
    "│   ├── eda.py\n",
    "├── output/\n",
    "│   ├── figures/\n",
    "│   └── reports/\n",
    "├── main.py\n",
    "├── requirements.txt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd2682d6-e567-4499-8129-c58990060d61",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "project/\n",
    "│\n",
    "├── data/\n",
    "│   ├── raw/            # Original immutable data\n",
    "│   ├── processed/      # Cleaned, transformed data\n",
    "│   └── outputs/       # Final results, models\n",
    "│\n",
    "├── notebooks/\n",
    "│   ├── 01_data_exploration.ipynb\n",
    "│   ├── 02_feature_engineering.ipynb\n",
    "│   └── 03_modeling.ipynb\n",
    "│\n",
    "├── src/\n",
    "│   ├── data_loading.py\n",
    "│   ├── data_cleaning.py\n",
    "│   ├── exploration.py\n",
    "│   ├── modeling.py\n",
    "│   └── utils.py\n",
    "│\n",
    "├── config/             # Configuration files\n",
    "│   └── params.yaml\n",
    "│\n",
    "├── docs/               # Documentation\n",
    "├── tests/              # Unit tests\n",
    "└── main.py             # Pipeline driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c8eb1f-5177-4a82-b8ed-dce23de15e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe769159-a4d2-4a99-b00e-ebbd3d768800",
   "metadata": {},
   "source": [
    "# project real time progress insight suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a467af-87e2-4116-8baf-1968ac1bc477",
   "metadata": {},
   "source": [
    "General project information summary\n",
    "1. Data Loading\n",
    "2. Data Familiarization\n",
    "3. Data Cleaning\n",
    "4. Data Transformation\n",
    "5. Exploratory Data Analysis (EDA)\n",
    "6. Descriptive Analysis\n",
    "7. Diagnostic Analysis\n",
    "8. Inferential Analysis\n",
    "9. Predictive Analysis\n",
    "10. Prescriptive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075abb61-d94a-4a0f-8154-f4ab6ce3e097",
   "metadata": {},
   "source": [
    "## Data Familiarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5726e0ae-bef3-4a45-b82f-0b8d1bea2c75",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Thank you for sharing your progress file: **`RetailNexus_Corp_analytics (1).html`**. I’ve reviewed the notebook content and can confirm that:\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ I Can Access:\n",
    "\n",
    "1. **All Python code cells**\n",
    "2. **All outputs of the cells**, including:\n",
    "\n",
    "   * `.head()`, `.info()`, `.describe()`\n",
    "   * Chart outputs (like `Sales` boxplots)\n",
    "   * Value counts and type introspections\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Overview of What You’ve Done So Far\n",
    "\n",
    "Here’s a structured analysis:\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧱 1. **Data Loading**\n",
    "\n",
    "* You've successfully loaded three key tables:\n",
    "\n",
    "  * `Orders_df`\n",
    "  * `People_df`\n",
    "  * `Returns_df`\n",
    "\n",
    "✅ Loaded correctly, with visual confirmation via `.head()`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧪 2. **Data Familiarization Phase**\n",
    "\n",
    "You implemented a simple but effective class `DataFamiliarization`, which includes:\n",
    "\n",
    "* `.show_info()` – Displays `.info()` summary\n",
    "* `.summary_statistics()` – Uses `.describe()`\n",
    "* `.show_valueCounts()` – For categorical distribution\n",
    "* `.show_chart()` – For simple visualizations\n",
    "\n",
    "✅ Outputs show that each function worked as intended.\n",
    "\n",
    "---\n",
    "\n",
    "#### 📉 3. **Chart Testing (e.g. Boxplot)**\n",
    "\n",
    "You attempted to visualize outliers in the `Sales` column:\n",
    "\n",
    "* The standard boxplot didn’t show a visible \"box\" due to skewness.\n",
    "* Value distribution is extremely **right-skewed**, which matches earlier observations (Q3 = \\~209, Max = 22,638)\n",
    "\n",
    "✅ Your efforts here are logically correct. Just needs scaling (e.g. `log1p`) for clearer visual.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🪛 4. **Issues Identified & Solved**\n",
    "\n",
    "* You debugged:\n",
    "\n",
    "  * Misused string literals (`'column_name'`)\n",
    "  * Wrong import (`matplotlib` instead of `matplotlib.pyplot as plt`)\n",
    "  * Boxplot formatting using `pandas` vs `matplotlib`\n",
    "  * Scaling issues due to outliers\n",
    "\n",
    "✅ Each problem was resolved step-by-step.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Suggestions to Improve\n",
    "\n",
    "| Area                     | Suggestion                                                            |\n",
    "| ------------------------ | --------------------------------------------------------------------- |\n",
    "| **Boxplot Clarity**      | Use `np.log1p(df[column])` or filter outliers to clarify the chart    |\n",
    "| **Class Reusability**    | Add method for multiple columns at once (batch profiling)             |\n",
    "| **Output Handling**      | Consider returning outputs (e.g. fig, stats) for saving/logging       |\n",
    "| **Chart Expansion**      | Add support for scatter plots and compare multiple metrics visually   |\n",
    "| **Profiling Automation** | Use libraries like `pandas_profiling` or `ydata-profiling` optionally |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Next Steps\n",
    "\n",
    "Would you like me to:\n",
    "\n",
    "1. Refactor your current class into a **cleaner professional version**?\n",
    "2. Add **outlier detection**, **column profiling**, or **chart export** functionality?\n",
    "3. Move on to the **data cleaning phase** with structured class setup?\n",
    "\n",
    "Let me know how you want to proceed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72245a3-b06d-41c4-8ed9-4fc57e4fe824",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Excellent. You're building your project in a structured and job-relevant way — exactly what employers love to see in beginner portfolios. Let’s now **bridge your Data Familiarization → Data Cleaning → Data Transformation** phases logically.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Objective\n",
    "\n",
    "Create **5–10 cleaning tasks** as beginner-friendly **methods** in a future `DataCleaning` class, which:\n",
    "\n",
    "* Are **realistic in industry settings**\n",
    "* Use **basic yet effective** techniques\n",
    "* Set you up for the next stage: **Data Transformation**\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Transition Context\n",
    "\n",
    "Your current dataset (Retail orders) has:\n",
    "\n",
    "* Missing values\n",
    "* Duplicates\n",
    "* Outliers (like in `Sales`)\n",
    "* Inconsistent data types\n",
    "* Categorical and numerical columns\n",
    "\n",
    "---\n",
    "\n",
    "## 🧼 Top 10 Cleaning Tasks (As Methods in Your `DataCleaning` Class)\n",
    "\n",
    "| Method Name                                 | Purpose                                                 | Notes                                     |\n",
    "| ------------------------------------------- | ------------------------------------------------------- | ----------------------------------------- |\n",
    "| `check_missing()`                           | Check total missing values per column                   | Use `df.isnull().sum()`                   |\n",
    "| `fill_missing(strategy='mean', columns=[])` | Fill missing values using mean, median, or mode         | Good beginner practice                    |\n",
    "| `drop_duplicates()`                         | Drop exact duplicate rows                               | Use `df.drop_duplicates()`                |\n",
    "| `fix_data_types()`                          | Convert columns to correct types (e.g. dates, numerics) | Useful before transformation              |\n",
    "| `remove_outliers(column)`                   | Remove outliers using IQR method                        | Already seen the need in `Sales`          |\n",
    "| `standardize_text(columns=[])`              | Lowercase, strip, and fix text inconsistencies          | For fields like `City`, `Segment`, etc.   |\n",
    "| `normalize_dates(columns=[])`               | Convert messy date strings into `datetime` objects      | Essential for time series                 |\n",
    "| `validate_ranges(column, min_val, max_val)` | Ensure numerical values fall within expected ranges     | Optional checks (e.g., quantity or sales) |\n",
    "| `clean_categorical(columns=[])`             | Clean inconsistent categorical values                   | Fix spacing, casing, etc.                 |\n",
    "| `generate_clean_report()`                   | Print a report of the cleaning done (basic logging)     | Great for job showcase/reporting          |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Transition to Transformation Phase\n",
    "\n",
    "Once you've cleaned:\n",
    "\n",
    "* Missing values → you can **impute**, **create indicators**, or **engineer features**\n",
    "* Dates → you can **extract months, years, days**\n",
    "* Categoricals → you can **encode**\n",
    "* Outliers → you can **scale or bin values**\n",
    "* Texts → ready for **grouping**, **mapping**, or **labeling**\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Bonus (Optional Cleaning-to-Transform Methods)\n",
    "\n",
    "| Method                                       | Role                                                            |\n",
    "| -------------------------------------------- | --------------------------------------------------------------- |\n",
    "| `create_flag_missing(columns)`               | Adds a binary column to track originally missing values         |\n",
    "| `create_binned_column(column, bins, labels)` | Useful in transformation but depends on cleaned numeric columns |\n",
    "| `map_categoricals(column, mapping_dict)`     | Helps normalize messy text fields for transformation            |\n",
    "\n",
    "---\n",
    "\n",
    "## 📘 Sample Template\n",
    "\n",
    "```python\n",
    "class DataCleaning:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "\n",
    "    def check_missing(self):\n",
    "        return self.df.isnull().sum()\n",
    "\n",
    "    def drop_duplicates(self):\n",
    "        self.df.drop_duplicates(inplace=True)\n",
    "\n",
    "    def fix_data_types(self):\n",
    "        # Example: Convert 'Order Date' to datetime\n",
    "        self.df['Order Date'] = pd.to_datetime(self.df['Order Date'], errors='coerce')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to help:\n",
    "\n",
    "1. Build this full `DataCleaning` class now?\n",
    "2. Proceed with the `DataTransformation` roadmap?\n",
    "3. Or finalize the refactored `DataFamiliarization` version for completeness?\n",
    "\n",
    "Let me know how you’d like to pace it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca1653-c95f-4312-b8c3-d21349442bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "485322c0-620e-403c-98a8-a43a53cffefc",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cb259b-d2a5-4883-9c24-5ee87a65d51a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Based on my review of your uploaded dataset (`RetailNexus_Corp_analytics (1).html`) and your progress so far, here's a **senior-level breakdown with beginner-friendly best practices** tailored to your project goals.\n",
    "\n",
    "You are building a clean, job-ready data science project, and these practices will make it both **pedagogically sound** and **professionally impressive**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 a. Handling Missing and Duplicate Values\n",
    "\n",
    "### 🔍 Options for Missing Values:\n",
    "\n",
    "| Option                | Method                  | Implication                                             | Recommended for You?                               |\n",
    "| --------------------- | ----------------------- | ------------------------------------------------------- | -------------------------------------------------- |\n",
    "| Remove Rows           | `df.dropna()`           | Loses data; only safe if <5% of rows are affected       | ✅ *Yes*, if the missing data is minimal and random |\n",
    "| Fill with Mean/Median | `df.fillna(df.mean())`  | Keeps rows, but may distort real distribution if skewed | ✅ *Yes*, for numerical columns                     |\n",
    "| Fill with Mode        | `df.fillna(df.mode())`  | Great for categoricals                                  | ✅ *Yes*, for fields like `Region` or `Segment`     |\n",
    "| Placeholder Fill      | `df.fillna('Unknown')`  | Good for non-numerical data like cities, customer info  | ✅ Optional, where mode doesn’t make sense          |\n",
    "| Custom Fill           | Based on business logic | Advanced, used in production                            | ❌ Not needed here                                  |\n",
    "\n",
    "### 🔍 Options for Duplicates:\n",
    "\n",
    "| Option               | Method                                                    | Implication                                      | Recommended for You?                         |\n",
    "| -------------------- | --------------------------------------------------------- | ------------------------------------------------ | -------------------------------------------- |\n",
    "| Drop Full Duplicates | `df.drop_duplicates()`                                    | Safe first step                                  | ✅ Yes                                        |\n",
    "| Drop Based on Subset | `df.drop_duplicates(subset=['Order ID', 'Product Name'])` | Use if only a specific combination is duplicated | ✅ Yes if you detect product-level redundancy |\n",
    "\n",
    "### ✅ Best Practice for You:\n",
    "\n",
    "* Use `.isnull().sum()` to **profile missingness**\n",
    "* Use **mode/mean** to fill where safe\n",
    "* Drop rows only if you lose <5%\n",
    "* Drop duplicates blindly only after profiling\n",
    "\n",
    "---\n",
    "\n",
    "## 📆 b. Fix / Normalize Date Columns\n",
    "\n",
    "### 🔍 Best Practices:\n",
    "\n",
    "| Task                | Method                                   | Why It Matters                               |\n",
    "| ------------------- | ---------------------------------------- | -------------------------------------------- |\n",
    "| Convert Dates       | `pd.to_datetime()`                       | Needed for trend analysis, EDA, and modeling |\n",
    "| Ensure Consistency  | Handle mixed date formats                | Some Excel exports store dates as text       |\n",
    "| Feature Engineering | Extract `Year`, `Month`, `Weekday`, etc. | Critical for time-based insights             |\n",
    "| Duration Features   | `(Ship Date - Order Date).days`          | Can show delivery delays (logistics KPI)     |\n",
    "\n",
    "### ✅ Best Practice for You:\n",
    "\n",
    "```python\n",
    "df['Order Date'] = pd.to_datetime(df['Order Date'], errors='coerce')\n",
    "df['Ship Date'] = pd.to_datetime(df['Ship Date'], errors='coerce')\n",
    "df['Shipping Delay'] = (df['Ship Date'] - df['Order Date']).dt.days\n",
    "```\n",
    "\n",
    "Perfect for your retail domain and very interview-relevant.\n",
    "\n",
    "---\n",
    "\n",
    "## 📉 c. Handle Outliers\n",
    "\n",
    "### 🔍 Options:\n",
    "\n",
    "| Method          | Use Case                         | Implication                                   | Recommended?                     |\n",
    "| --------------- | -------------------------------- | --------------------------------------------- | -------------------------------- |\n",
    "| IQR Rule        | Detect and remove                | Most interpretable, good for boxplot outliers | ✅ Yes                            |\n",
    "| Z-Score         | Detect                           | Good for normally distributed data            | ❌ No (your sales data is skewed) |\n",
    "| Log Transform   | Keep outliers but reduce skew    | Best for visualizations and models            | ✅ Yes                            |\n",
    "| Cap (Winsorize) | Cap max/min values to percentile | Keeps all rows but limits extremes            | Optional                         |\n",
    "\n",
    "### ✅ Best Practice for You:\n",
    "\n",
    "Use **IQR + Log Transform** on `Sales`, `Profit`, and `Discount`:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Log Transform\n",
    "df['Sales_log'] = np.log1p(df['Sales'])\n",
    "\n",
    "# IQR Removal\n",
    "Q1 = df['Profit'].quantile(0.25)\n",
    "Q3 = df['Profit'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df = df[(df['Profit'] >= Q1 - 1.5*IQR) & (df['Profit'] <= Q3 + 1.5*IQR)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔤 d. Standardize Text Columns\n",
    "\n",
    "### 🔍 Why Important:\n",
    "\n",
    "* Improves matching, grouping, joins\n",
    "* Reduces feature explosion when encoding\n",
    "\n",
    "### 🔧 Best Actions:\n",
    "\n",
    "| Task                | Example                                   | Why                                       |\n",
    "| ------------------- | ----------------------------------------- | ----------------------------------------- |\n",
    "| Lowercase All       | `df[col] = df[col].str.lower()`           | Avoids `'Consumer'` vs `'consumer'` issue |\n",
    "| Strip Spaces        | `df[col] = df[col].str.strip()`           | Removes trailing/leading errors           |\n",
    "| Replace Underscores | `df[col] = df[col].str.replace('_', ' ')` | Clean labels                              |\n",
    "| Remove Duplicates   | Fix values like `'South '` and `'south'`  | Consistent grouping                       |\n",
    "\n",
    "✅ Best for columns like: `City`, `State`, `Segment`, `Category`, `Sub-Category`\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 e. Generate Report\n",
    "\n",
    "### 🔍 What to Include:\n",
    "\n",
    "| Report Section        | Tool/Code                    | Purpose                                 |\n",
    "| --------------------- | ---------------------------- | --------------------------------------- |\n",
    "| Missing Value Summary | `.isnull().sum()`            | Justify how you cleaned                 |\n",
    "| Duplicate Check       | `.duplicated().sum()`        | Show that duplicates were handled       |\n",
    "| Outlier Count         | IQR or histogram plot        | Before & after cleaning                 |\n",
    "| Data Type Fix Log     | Use a print or markdown cell | Show what was converted and why         |\n",
    "| Shape Before/After    | `df.shape`                   | Show cleaning didn’t overly reduce data |\n",
    "\n",
    "✅ For job applications, show this in a **markdown cell** or print statements at the end of the class like:\n",
    "\n",
    "```python\n",
    "def generate_cleaning_report(self):\n",
    "    print(\"✅ Cleaning Summary Report\")\n",
    "    print(f\"Missing values handled in: {self.missing_columns}\")\n",
    "    print(f\"Duplicates removed: {self.duplicates_removed}\")\n",
    "    print(f\"Data shape now: {self.df.shape}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary: Best Practice Setup for You\n",
    "\n",
    "| Task              | Recommended Action                           |\n",
    "| ----------------- | -------------------------------------------- |\n",
    "| Handle missing    | Use `.fillna()` with mode/mean; drop if <5%  |\n",
    "| Handle duplicates | Use `drop_duplicates()`                      |\n",
    "| Fix dates         | Use `pd.to_datetime()` + feature engineering |\n",
    "| Handle outliers   | IQR removal + log transform                  |\n",
    "| Standardize text  | `.str.lower().strip()`                       |\n",
    "| Generate report   | Capture actions and changes made             |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to now:\n",
    "\n",
    "* Build the `DataCleaning` class from these methods?\n",
    "* Continue with `DataTransformation` phase?\n",
    "* Or integrate all into a modular project format?\n",
    "\n",
    "Let me know your next priority.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc40a95e-75b9-47af-9eeb-b722f419cf14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb1a20-bf36-4468-8209-17197be292c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b64fa-bb91-4402-8141-61cf4df877ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f3a3df8-302c-4ff1-8f6a-051d9dd6d7e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# suggested tasks of phases of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e667dd9a-add7-4fef-a1ad-ebee02ecd4f5",
   "metadata": {},
   "source": [
    "## 10 phases of your data science project workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04751841-c2dd-4d87-8366-d4c13b58af54",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Certainly! Below is a conceptual discussion of each of the 10 phases of your\n",
    "data science project workflow. This overview is aligned with **real-world industry standards** and can guide your planning and execution.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 1. **Data Loading**\n",
    "\n",
    "**Concept:**\n",
    "Data loading is the initial step where raw data is ingested into your working environment (e.g., Jupyter, VSCode) from local files, databases, cloud storage, APIs, etc.\n",
    "\n",
    "**Typical Tasks:**\n",
    "\n",
    "* Read multiple file formats (CSV, Excel, JSON).\n",
    "* Handle encoding issues.\n",
    "* Set correct data types.\n",
    "* Load multiple related tables (e.g., `orders`, `people`, `returns`).\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "* Use memory-efficient reading (`dtype` and `usecols`).\n",
    "* Modularize file paths and configs.\n",
    "* Validate loading success (row counts, null checks).\n",
    "\n",
    "**References:**\n",
    "\n",
    "* McKinney, Wes. *Python for Data Analysis*. Chapter 6 (High accuracy: ✅✅✅).\n",
    "\n",
    "---\n",
    "\n",
    "## 🧾 2. **Data Familiarization**\n",
    "\n",
    "**Concept:**\n",
    "This phase is about understanding the structure, content, and semantics of the data. It’s exploratory, not analytical.\n",
    "\n",
    "**Typical Tasks:**\n",
    "\n",
    "* View data types, column names, and value ranges.\n",
    "* Check class balance and categorical distributions.\n",
    "* Profile summary statistics and sample rows.\n",
    "* Assess dimensionality, keys, and join logic (e.g., how `orders` relates to `returns` and `people`).\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "* Use profiling libraries like `pandas_profiling` or `sweetviz`.\n",
    "* Keep a log of findings for future phases.\n",
    "\n",
    "**References:**\n",
    "\n",
    "* *Data Science for Business* by Provost & Fawcett. Chapter 2 (High accuracy: ✅✅✅).\n",
    "\n",
    "---\n",
    "\n",
    "## 🧹 3. **Data Cleaning**\n",
    "\n",
    "**Concept:**\n",
    "Prepare the dataset by correcting errors, inconsistencies, and quality issues that can distort analysis or modeling.\n",
    "\n",
    "**Typical Tasks:**\n",
    "\n",
    "* Handle missing values.\n",
    "* Fix incorrect data (e.g., wrong date formats).\n",
    "* Remove duplicates.\n",
    "* Normalize inconsistent values (e.g., USA vs U.S.A.).\n",
    "* Validate and fix relational integrity (e.g., all `Order ID`s in `returns` exist in `orders`).\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "* Always preserve a raw copy.\n",
    "* Create a cleaning log or audit trail.\n",
    "* Use assertions/tests to confirm fixes.\n",
    "\n",
    "**References:**\n",
    "\n",
    "* Hadley Wickham's *Tidy Data* paper (Medium-high accuracy: ✅✅).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 4. **Data Transformation**\n",
    "\n",
    "**Concept:**\n",
    "Modify or engineer features to make the data more useful for analysis or modeling.\n",
    "\n",
    "**Typical Tasks:**\n",
    "\n",
    "* Feature engineering (e.g., create `Delivery Time = Ship Date - Order Date`).\n",
    "* Encoding categorical variables.\n",
    "* Normalizing or scaling.\n",
    "* Aggregation, binning, and pivoting.\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "* Use pipelines (`sklearn.pipeline`) to ensure reproducibility.\n",
    "* Modularize reusable transformations.\n",
    "\n",
    "**References:**\n",
    "\n",
    "* Géron, Aurélien. *Hands-On ML with Scikit-Learn*. Chapter 2–3 (High accuracy: ✅✅✅).\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 5. **Exploratory Data Analysis (EDA)**\n",
    "\n",
    "**Concept:**\n",
    "Perform visual and statistical analysis to detect trends, patterns, and relationships.\n",
    "\n",
    "**Typical Tasks:**\n",
    "\n",
    "* Univariate and bivariate plots (histograms, bar charts, scatter plots).\n",
    "* Correlation matrix and heatmaps.\n",
    "* Outlier detection (via boxplots or statistical tests).\n",
    "* Drill down by dimensions (segment, region, etc.).\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "* Create clear visualizations with titles/labels.\n",
    "* Document insights in markdown or notebooks.\n",
    "\n",
    "**References:**\n",
    "\n",
    "* Tukey, John. *Exploratory Data Analysis* (Foundational; High accuracy: ✅✅✅).\n",
    "\n",
    "---\n",
    "\n",
    "## 📉 6. **Descriptive Analysis**\n",
    "\n",
    "**Concept:**\n",
    "Summarize data using statistical metrics and business-relevant aggregations.\n",
    "\n",
    "**Typical Tasks:**\n",
    "\n",
    "* Compute central tendencies (mean, median).\n",
    "* Group-level statistics (e.g., total sales per region).\n",
    "* Pareto analysis (80/20 rule).\n",
    "* Cross-tabulations and frequency analysis.\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "* Use visual + tabular outputs.\n",
    "* Tailor KPIs to business goals (e.g., profitability, return rates).\n",
    "\n",
    "**References:**\n",
    "\n",
    "* Dorian Pyle’s *Business Modeling and Data Mining* (Medium-high accuracy: ✅✅).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔎 7. **Diagnostic Analysis**\n",
    "\n",
    "**Concept:**\n",
    "Investigate *why* something happened using statistical relationships and data patterns.\n",
    "\n",
    "**Typical Tasks:**\n",
    "\n",
    "* Root cause analysis (RCA).\n",
    "* Correlation vs causation checks.\n",
    "* Trend comparisons (before/after, this vs that).\n",
    "* Drill-down dashboards for problem investigation.\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "* Use hypothesis-driven approach.\n",
    "* Avoid confirmation bias.\n",
    "* Combine with subject-matter knowledge.\n",
    "\n",
    "**References:**\n",
    "\n",
    "* Harvard Business Review: “A Refresher on Root Cause Analysis” (Medium-high accuracy: ✅✅).\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 8. **Inferential Analysis**\n",
    "\n",
    "**Concept:**\n",
    "Draw conclusions about a population based on sample data using probability theory.\n",
    "\n",
    "**Typical Tasks:**\n",
    "\n",
    "* Hypothesis testing (e.g., T-tests, chi-square).\n",
    "* Confidence intervals.\n",
    "* Statistical significance for metrics (e.g., is average profit different between segments?).\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "* Pre-define significance thresholds (e.g., alpha = 0.05).\n",
    "* Report p-values with effect sizes.\n",
    "\n",
    "**References:**\n",
    "\n",
    "* Field, Andy. *Discovering Statistics Using Python*. (High accuracy: ✅✅✅)\n",
    "\n",
    "---\n",
    "\n",
    "## 🤖 9. **Predictive Analysis**\n",
    "\n",
    "**Concept:**\n",
    "Use statistical and ML models to forecast future events or classify unseen data.\n",
    "\n",
    "**Typical Tasks:**\n",
    "\n",
    "* Train/test ML models (e.g., regression, classification).\n",
    "* Model evaluation (RMSE, accuracy, AUC).\n",
    "* Cross-validation and hyperparameter tuning.\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "* Split into training/validation/test sets.\n",
    "* Use pipelines and `GridSearchCV`.\n",
    "\n",
    "**References:**\n",
    "\n",
    "* Géron, *Hands-On ML with Scikit-Learn, Keras, and TensorFlow*. (High accuracy: ✅✅✅)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 10. **Prescriptive Analysis**\n",
    "\n",
    "**Concept:**\n",
    "Suggest optimal actions or decisions using models, simulations, or optimization techniques.\n",
    "\n",
    "**Typical Tasks:**\n",
    "\n",
    "* Scenario and what-if analysis.\n",
    "* Constraint-based optimization (e.g., linear programming).\n",
    "* Decision trees and simulations.\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "* Tie outputs to business objectives (e.g., maximize profit under constraints).\n",
    "* Present actionable recommendations.\n",
    "\n",
    "**References:**\n",
    "\n",
    "* IBM SPSS Prescriptive Analytics Guide (Medium-high accuracy: ✅✅).\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like to begin designing the class structure for Phase 1 (📥 Data Loading), or if you'd like a visual workflow summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf050241-b929-4772-965f-d4a3f05830f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b6e6a21-14db-4a8e-bd9e-d5456f6db0d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 📘 Important project Insights – With Reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c563db-c4ca-4f66-a249-3b961c3e8f42",
   "metadata": {},
   "source": [
    "## 📘 Important Columns for Insights – With Reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb2c9a-f460-42a3-8933-2643abd70250",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Great question. Even though no matching text was retrieved directly from the uploaded `.html` dataset preview, I’ve already studied your dataset structure from our earlier review of the `RetailNexus_Corp_analytics.html`. So I’ll give a reliable analysis based on what’s present in your **Orders\\_df** from the Superstore dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 📘 a. **Important Columns for Insights – With Reasons**\n",
    "\n",
    "These columns are crucial when extracting actionable insights:\n",
    "\n",
    "| Column                          | Why It’s Important                                                                            |\n",
    "| ------------------------------- | --------------------------------------------------------------------------------------------- |\n",
    "| `Sales`                         | 💰 Core business metric; used to track revenue trends, identify top/bottom products/customers |\n",
    "| `Profit`                        | 📈 Determines profitability; helps segment successful vs loss-making categories or regions    |\n",
    "| `Quantity`                      | 📦 Useful for identifying sales volume vs revenue; important for operations analysis          |\n",
    "| `Category` / `Sub-Category`     | 🗃 Used to group and analyze what’s selling and profitable by product type                    |\n",
    "| `Segment`                       | 🧍‍♂️ Helps identify customer group behavior (e.g., Corporate vs Consumer vs Home Office)     |\n",
    "| `Region` / `City` / `State`     | 🌍 Key for geo-insights — where sales/profit is growing or declining                          |\n",
    "| `Order Date` / `Ship Date`      | 📅 Temporal analysis — time-based trends, delays, seasonality                                 |\n",
    "| `Discount`                      | 🏷️ Used to analyze correlation with Profit or Sales and cannibalization effects              |\n",
    "| `Customer Name` / `Customer ID` | 🧑 Helps in RFM or churn analysis for customer segmentation                                   |\n",
    "| `Order ID`                      | 🔎 Useful for joining tables, tracing transactions, or order-level analytics                  |\n",
    "\n",
    "---\n",
    "\n",
    "## 🤖 b. **Which Columns Could Be Target Variables for ML – With Reasons**\n",
    "\n",
    "Here are some ML-compatible targets based on different tasks:\n",
    "\n",
    "| Target Column                  | ML Use Case       | Reason                                                                                |\n",
    "| ------------------------------ | ----------------- | ------------------------------------------------------------------------------------- |\n",
    "| `Profit`                       | 🔢 Regression     | Predict profitability of future transactions (continuous target)                      |\n",
    "| `Sales`                        | 🔢 Regression     | Estimate revenue based on features like Category, Region, etc.                        |\n",
    "| `Ship Mode`                    | 🔠 Classification | Predict best shipping method given product type, region, and urgency                  |\n",
    "| `Returned` (from `Returns_df`) | ❌ Classification  | Predict likelihood of product return — useful for customer service or fraud detection |\n",
    "| `Segment`                      | 🔠 Classification | Predict customer segment from past behavior (helps in personalization)                |\n",
    "| `Category` / `Sub-Category`    | 🔠 Classification | Predict product type from order metadata (less common, but possible)                  |\n",
    "\n",
    "---\n",
    "\n",
    "### 🚫 Columns **Not** Suitable as Target:\n",
    "\n",
    "| Column                              | Reason                                                   |\n",
    "| ----------------------------------- | -------------------------------------------------------- |\n",
    "| `Order ID`, `Customer ID`, `Row ID` | Identifiers, no predictive value                         |\n",
    "| `Order Date`, `Ship Date`           | Temporal metadata, better used as features or time index |\n",
    "| `City`, `State`, `Region`           | Better used as features to predict other outcomes        |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Suggested Beginner ML Problem\n",
    "\n",
    "**🎯 Target:** `Profit`\n",
    "**📊 Task:** Regression\n",
    "**📥 Features:** `Sales`, `Quantity`, `Discount`, `Category`, `Segment`, `Region`, `Ship Mode`\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to now help design:\n",
    "\n",
    "* A beginner-friendly ML pipeline using this dataset?\n",
    "* Or continue with the Data Cleaning and Transformation classes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774507af-af20-47e0-8d34-2ae02b584ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ceeff5a-918b-4828-b6b9-dea5b28dc514",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Project titles & company names suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb664815-265f-4bba-90c3-d0da4c54ff80",
   "metadata": {},
   "source": [
    "## 30 professional and compelling project title suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08415daa-e92a-4fc0-8de6-df8b5f52275d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Here are **15 professional and compelling project title suggestions** for your 10-phase, multi-table data science project. The titles are crafted to highlight your data science competencies, business relevance, and technical depth — ideal for a portfolio or resume.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Business-Oriented Titles**\n",
    "\n",
    "1. **Optimizing Retail Performance: A Multi-Phase Data Science Workflow from Raw Data to Recommendations**\n",
    "2. **Sales Intelligence and Returns Optimization for U.S. Retail Operations**\n",
    "3. **Full-Cycle Data Science Project: Improving Profitability Through Multi-Table Retail Analysis**\n",
    "4. **From Orders to Insights: A Data-Driven Study of Retail Sales and Customer Segments**\n",
    "5. **Driving Business Value through Prescriptive Analytics on U.S. Retail Transactions**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Technical Focus Titles**\n",
    "\n",
    "6. **A Modular Python Pipeline for End-to-End Retail Data Science: Loading, Cleaning, and Modeling**\n",
    "7. **Multi-Table Data Integration and Analysis in Retail: A Class-Based Python Implementation**\n",
    "8. **Data Science at Scale: Building Reusable Analysis Classes Across the Data Lifecycle**\n",
    "9. **Designing a Reproducible Data Science Architecture for Retail Sales Optimization**\n",
    "10. **Structured Python Classes for End-to-End Retail Data Insights: From Loading to Prescriptive Modeling**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Academic/Portfolio Framing Titles**\n",
    "\n",
    "11. **Capstone Project: A Reproducible Framework for Multi-Phase Data Science Using Python**\n",
    "12. **Applied Data Science Project on U.S. Retail Sales: A Lifecycle Approach**\n",
    "13. **From Raw Tables to Business Impact: A 10-Phase Analytical Pipeline in Python**\n",
    "14. **A Portfolio-Ready Full Stack Data Science Project on Retail Analytics**\n",
    "15. **Data-Driven Decision Making Using Retail Data: Insights from a 10-Stage Analytical Framework**\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Recommendation:\n",
    "\n",
    "If your goal is to showcase the **structure and completeness of your data science process**, use titles like:\n",
    "\n",
    "> **\"Full-Cycle Retail Analytics: A Reproducible Python Project from Data Loading to Prescriptive Analysis\"**\n",
    "\n",
    "If you want to highlight **business impact and value**, go with:\n",
    "\n",
    "> **\"Optimizing Profitability and Customer Insights in Retail Through Data Science\"**\n",
    "\n",
    "Would you like help picking one based on a specific job role (e.g., Data Analyst vs Data Scientist vs BI Analyst) or employer (e.g., NHS, e-commerce, consultancy)?\n",
    "\n",
    "\n",
    "Excellent — based on your dataset, which resembles the popular **Superstore Dataset** used in retail analytics, I’ve studied the structure across the three tables:\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Tables Summary:\n",
    "\n",
    "### 1. **Orders Table (Main Table)**\n",
    "\n",
    "Retail transaction-level data including:\n",
    "\n",
    "* Product (category, sub-category, sales, discount, profit)\n",
    "* Customer (segment, region, city)\n",
    "* Order & shipping details\n",
    "\n",
    "### 2. **People Table**\n",
    "\n",
    "* Region-wise assignment of managers or reps (possibly for returns or performance mapping)\n",
    "\n",
    "### 3. **Returns Table**\n",
    "\n",
    "* Order ID + return status (\"Yes\") → used to flag returns and investigate loss, fraud, or performance issues.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 10 Professional Project Titles (Employer-Attractive)\n",
    "\n",
    "Here are **10 refined, real-world-friendly titles** you can confidently include in a portfolio, CV, or GitHub:\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Data Analysis & Insights\n",
    "\n",
    "1. **\"Retail Order Analytics: Identifying Regional Sales Trends and Customer Segments for Growth Strategy\"**\n",
    "   *Focus: Segment-wise sales, product-wise revenue, top regions.*\n",
    "\n",
    "2. **\"Returns and Profitability in E-Commerce: Analyzing Product Returns Impact on Business Performance\"**\n",
    "   *Focus: Join with `returns`, analyze loss-making segments.*\n",
    "\n",
    "3. **\"Order Fulfillment Analysis: Shipping Mode Efficiency and Delay Detection in Retail Logistics\"**\n",
    "   *Focus: Compare order vs ship date, per shipping mode.*\n",
    "\n",
    "4. **\"Geospatial Sales Performance: U.S. State-Level Retail Insights Using Python and Plotly\"**\n",
    "   *Focus: Map-based visuals; city/state-based profits.*\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 Predictive / Optimization Focus\n",
    "\n",
    "5. **\"Profit Optimization in Retail: A Discount Strategy Analysis Using Exploratory and Statistical Methods\"**\n",
    "   *Focus: How discounts affect profit (negative correlation); identify optimal discount ranges.*\n",
    "\n",
    "6. **\"Forecasting Retail Sales by Sub-Category Using Time-Series Aggregation and Decomposition\"**\n",
    "   *Focus: Convert `Order Date` to monthly/quarterly trend.*\n",
    "\n",
    "7. **\"Product Recommendation Trends: Identifying Top-Selling Sub-Categories by Region and Segment\"**\n",
    "   *Focus: Cross-tabulate product-category/region patterns.*\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 Advanced + Business-Focused\n",
    "\n",
    "8. **\"Manager Performance and Regional Returns: Analyzing Sales-to-Returns Ratio Across Business Territories\"**\n",
    "   *Focus: Merge `People` and `Returns` tables to evaluate who manages most returns.*\n",
    "\n",
    "9. **\"Customer Value Segmentation: Using RFM (Recency, Frequency, Monetary) Analysis to Identify High-Value Clients\"**\n",
    "   *Focus: Advanced segmentation technique based on purchase behavior.*\n",
    "\n",
    "10. **\"Profit Loss Attribution: Identifying Drivers of Negative Profit Orders in Retail Transactions\"**\n",
    "    *Focus: Deep-dive into transactions with discounts > 0.3 and loss in profit.*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 Tips for Using These Titles Professionally\n",
    "\n",
    "| Tip                                                                                   | Why It Helps                   |\n",
    "| ------------------------------------------------------------------------------------- | ------------------------------ |\n",
    "| Use verbs like **\"Analyzing\"**, **\"Optimizing\"**, **\"Forecasting\"**                   | Signals action + value         |\n",
    "| Highlight **data context** (e.g., retail, shipping, returns)                          | Shows domain awareness         |\n",
    "| Tie to **business goals** (e.g., profit, logistics, customer value)                   | Employers love business impact |\n",
    "| Include **tools/tech** in project README (e.g., Python, Pandas, Plotly, Scikit-learn) | Shows technical depth          |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to now generate:\n",
    "\n",
    "* 📘 A sample README for one of these titles?\n",
    "* 🧠 A full breakdown of project steps (like EDA → Modeling → Reporting)?\n",
    "* 🔧 Python starter code for joining and preparing these tables?\n",
    "\n",
    "Let me know how you want to proceed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a2637a-b92f-4dd8-ab1d-7de45b98cfa0",
   "metadata": {},
   "source": [
    "## 5 fictional but professional-sounding company names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b86a96-90e6-42a4-9b39-10b630e4e230",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Here are **5 fictional but professional-sounding company names** you can use in your project presentation or resume to simulate a real-world scenario. Each is designed to align with retail, analytics, or logistics-focused contexts to match your dataset:\n",
    "\n",
    "---\n",
    "\n",
    "### 🛒 Retail / E-Commerce Analytics Theme\n",
    "\n",
    "1. **OptiMart Analytics**\n",
    "\n",
    "   * *A retail intelligence company specializing in multi-channel sales optimization and customer segmentation.*\n",
    "\n",
    "2. **NovaCommerce Insights**\n",
    "\n",
    "   * *Provides actionable insights for global e-commerce businesses using advanced analytics.*\n",
    "\n",
    "3. **Quantira Retail Solutions**\n",
    "\n",
    "   * *A data-driven consultancy optimizing inventory, sales, and customer experience across retail chains.*\n",
    "\n",
    "---\n",
    "\n",
    "### 📦 Logistics / Returns / Operations Focus\n",
    "\n",
    "4. **TrackSphere Analytics**\n",
    "\n",
    "   * *Specializes in supply chain, returns analysis, and shipping performance for logistics-heavy businesses.*\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 General Data Science / Business Intelligence\n",
    "\n",
    "5. **InsightGrid Solutions**\n",
    "\n",
    "   * *A full-stack data science firm delivering business optimization through lifecycle analytics.*\n",
    "\n",
    "---\n",
    "\n",
    "These names can be customized depending on your industry focus. If you're targeting **healthcare, education, fintech, or energy**, I can generate names tailored to those domains too. Would you like a logo or branding to go along with one?\n",
    "\n",
    "\n",
    "Here are **5 professional and realistic company names** you can use to present your project as a case study or portfolio simulation. Each name is designed to sound **plausible**, industry-aligned, and suitable for **retail, e-commerce, or logistics analytics**—matching the style of your dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏢 Suggested Company Names\n",
    "\n",
    "| #   | Company Name           | Description                                                                                        |\n",
    "| --- | ---------------------- | -------------------------------------------------------------------------------------------------- |\n",
    "| 1️⃣ | **RetailNexus Corp**   | A national retail chain specializing in office supplies and home furnishings.                      |\n",
    "| 2️⃣ | **OmniCart Solutions** | An e-commerce platform offering multi-category products with nationwide delivery.                  |\n",
    "| 3️⃣ | **TradeBridge Retail** | A logistics-aware retail firm focused on optimized shipping and customer segmentation.             |\n",
    "| 4️⃣ | **MarketTrail Inc.**   | A data-driven retailer with regional managers and performance-based operations.                    |\n",
    "| 5️⃣ | **QuantiaMart**        | A customer-centric online store using analytics to improve sales, discounts, and returns handling. |\n",
    "\n",
    "---\n",
    "\n",
    "These names can be used in:\n",
    "\n",
    "* Your GitHub project title and README (e.g., “Sales Analysis for OmniCart Solutions”)\n",
    "* Presentation decks\n",
    "* Data storytelling reports or dashboards (Power BI/Tableau)\n",
    "\n",
    "Would you like me to generate a **project banner, logo design prompts**, or a **project summary using one of these names**?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd3d9b3-ccc2-4c1d-90df-ac3b8d41f5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed34a50-f57f-4b30-995f-7ef8bf11e74b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92eb1f-1ecc-4ae4-9038-bc9ad9c4392f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
